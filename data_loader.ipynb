{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381a9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import heapq\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdfffcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arg:\n",
    "    '''\n",
    "    Mock arguments\n",
    "    '''\n",
    "    def __init__(self, ratio, n_interact, n_neighbors, dataset, mission, neighbor_sample_size):\n",
    "        self.ratio = ratio\n",
    "        self.n_interact = n_interact\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.dataset = dataset\n",
    "        self.mission = mission\n",
    "        self.neighbor_sample_size = neighbor_sample_size\n",
    "        \n",
    "args = Arg(1, 20, 20, 'movie', 'gender', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d27bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating(args):\n",
    "    print('reading rating file ...')\n",
    "\n",
    "    # reading rating file\n",
    "    if args.mission == 'gender':\n",
    "        rating_file = './data/' + args.dataset + '/ratings_final_pos'\n",
    "        user_file = './data/' + args.dataset + '/user_gender_final'\n",
    "    elif args.mission == 'age':\n",
    "        rating_file = './data/' + args.dataset + '/age/ratings_final_pos'\n",
    "        user_file = './data/' + args.dataset + '/age/user_age_final'\n",
    "    if os.path.exists(rating_file + '.npy'):\n",
    "        rating_np = np.load(rating_file + '.npy')\n",
    "    else:\n",
    "        rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int64)\n",
    "        np.save(rating_file + '.npy', rating_np)\n",
    "    if os.path.exists(user_file + '.npy'):\n",
    "        users_np = np.load(user_file + '.npy')\n",
    "    else:\n",
    "        users_np = np.loadtxt(user_file + \".txt\", dtype=np.int64)\n",
    "        np.save(user_file + '.npy', users_np)\n",
    "    n_user = len(set(rating_np[:, 0]))  # 用户数\n",
    "    n_item = len(set(rating_np[:, 1]))  # 交互项目数\n",
    "    n_classes = len(set(users_np[:, 1]))  # 待分类的类别数（例如如果对性别分类，就是男女2类）\n",
    "    train_data, eval_data, test_data, adj_item, adj_user = dataset_split(n_user, users_np, rating_np, args)\n",
    "    np.random.shuffle(rating_np)\n",
    "\n",
    "    return n_user, n_item, train_data, eval_data, test_data, adj_item, n_classes, rating_np, adj_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8be4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(n_user, users_np, rating_np, args):\n",
    "    print('splitting dataset ...')\n",
    "\n",
    "    # train:eval:test = 7:1.5:1.5\n",
    "    eval_ratio = 0.15  # Validation Set\n",
    "    test_ratio = 0.15  # Testing Set\n",
    "\n",
    "    '''\n",
    "    以下代码处理用户数据即user\n",
    "    '''\n",
    "    eval_indices = np.random.choice(list(range(n_user)), size=int(n_user * eval_ratio), replace=False)  # 随机抽取\n",
    "    left = set(range(n_user)) - set(eval_indices)  # 做差集得到剩余下标集合\n",
    "    test_indices = np.random.choice(list(left), size=int(n_user * test_ratio), replace=False)\n",
    "    train_indices = list(left - set(test_indices))\n",
    "    if args.ratio < 1:\n",
    "        # 如果在参数中指定了training set的比例，则按照比例进一步挑选剩余的下标\n",
    "        # 否则默认把剩下的所有下标作为training set\n",
    "        train_indices = np.random.choice(list(train_indices), size=int(len(train_indices) * args.ratio), replace=False)\n",
    "\n",
    "    # 获取对应的数据\n",
    "    train_data = users_np[train_indices]\n",
    "    eval_data = users_np[eval_indices]\n",
    "    test_data = users_np[test_indices]\n",
    "\n",
    "    '''\n",
    "    以下代码处理rating\n",
    "    '''\n",
    "    # 构建pandas df\n",
    "    rating_df = pd.DataFrame(rating_np)\n",
    "    rating_df.columns = ['userid', 'movieid', 'gender']\n",
    "\n",
    "    # 从rating_np中分别取出train validation和test集合的中随机到的用户对应的下标\n",
    "    train_idx = []\n",
    "    for user in train_data[:, 0]:\n",
    "        # 这样写的原因是在rating中 一个用户id可以对应多个item\n",
    "        train_idx.extend(rating_df[rating_df['userid'] == user].index)\n",
    "    eval_idx = []\n",
    "    for user in eval_data[:, 0]:\n",
    "        eval_idx.extend(rating_df[rating_df['userid'] == user].index)\n",
    "    test_idx = []\n",
    "    for user in test_data[:, 0]:\n",
    "        test_idx.extend(rating_df[rating_df['userid'] == user].index)\n",
    "\n",
    "    # 取出对应的行\n",
    "    train_rating = rating_df.iloc[train_idx, :].values\n",
    "    eval_rating = rating_df.iloc[eval_idx, :].values\n",
    "    test_rating = rating_df.iloc[test_idx, :].values\n",
    "\n",
    "    np.random.shuffle(train_rating)\n",
    "    np.random.shuffle(eval_rating)\n",
    "    np.random.shuffle(test_rating)\n",
    "\n",
    "    # 构建用户-交互词典与交互-用户词典\n",
    "    userPos = dict()  # 用户-交互词典\n",
    "    itemUser = dict()  # 交互-用户词典\n",
    "    for i in range(rating_np.shape[0]):\n",
    "        user = rating_np[i, 0]  # 用户id\n",
    "        item = rating_np[i, 1]  # 交互item\n",
    "        if user not in userPos:\n",
    "            userPos[user] = set()\n",
    "        userPos[user].add(item)\n",
    "        if item not in itemUser:\n",
    "            itemUser[item] = set()\n",
    "        itemUser[item].add(user)\n",
    "    # 创建邻接矩阵(arg.n_interact -> number of item to be sampled 根据论文 为20)\n",
    "    adj_item = np.zeros([max(list(userPos.keys())) + 1, args.n_interact], dtype=np.int64)\n",
    "\n",
    "    for user in userPos:\n",
    "        interItem = list(userPos[user])\n",
    "        n_inter = len(interItem)\n",
    "        if n_inter > 0:\n",
    "            if n_inter >= args.n_interact:\n",
    "                # replace: Whether the sample is with or without replacement. Default is True, meaning that a value\n",
    "                # of a can be selected multiple times.\n",
    "                sampled_indices = np.random.choice(list(range(n_inter)), size=args.n_interact, replace=False)\n",
    "            else:\n",
    "                sampled_indices = np.random.choice(list(range(n_inter)), size=args.n_interact, replace=True)\n",
    "        adj_item[user] = np.array([interItem[i] for i in sampled_indices])\n",
    "\n",
<<<<<<< HEAD
    "    # 相邻用户计数矩阵(O(m*n*A_n^2))\n",
=======
    "    # 相邻用户计数矩阵(O(m*n*A_n^2)) 每一对用户与多少个相同的项目产生过交互\n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "    adj_user_count = np.zeros([max(list(userPos.keys())) + 1, max(list(userPos.keys())) + 1], dtype=np.int64)\n",
    "    for item in itemUser:\n",
    "        commenUsers = list(itemUser[item])  # 与item有关的所有用户\n",
    "        commenUsersSet = set(commenUsers)\n",
    "        for user in commenUsers:\n",
    "            # 对该item所有关联的user进行迭代\n",
    "            tmp = commenUsersSet\n",
    "            tmp.remove(user)  # 删除自身\n",
    "            for userj in list(tmp):\n",
    "                # 该用户与其他用户为相邻用户（只要跟同一个item相连的就是相邻用户）\n",
    "                adj_user_count[user, userj] += 1\n",
    "\n",
<<<<<<< HEAD
    "    idx = []\n",
=======
    "    idx = [] # idx[i][j]与user[i]产生相同交互第j多的用户id\n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "    for i in range(len(adj_user_count)):\n",
    "        # 用堆排序获取近邻最多的user的下标\n",
    "        # n_neighbors -> number of users.dat neighbors 根据论文近邻用户采样应该为10\n",
    "        tmp = heapq.nlargest(args.n_neighbors, range(len(adj_user_count[i])), adj_user_count[i].take)\n",
    "        idx.append(tmp)\n",
    "\n",
    "    # 创建邻接用户矩阵\n",
    "    adj_user = np.zeros([max(list(userPos.keys())) + 1, args.n_neighbors], dtype=np.int64)\n",
    "    for i, users in enumerate(idx):\n",
    "        n_neighbors = len(users)\n",
    "        if n_neighbors > 0:\n",
    "            if n_neighbors >= args.n_neighbors:\n",
    "                sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.n_neighbors, replace=False)\n",
    "            else:\n",
    "                sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.n_neighbors, replace=True)\n",
<<<<<<< HEAD
    "        adj_user[i] = np.array([users[i] for i in sampled_indices])\n",
=======
    "        adj_user[i] = np.array([users[i] for i in sampled_indices]) # 对邻近最多的用户进行采样\n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "    return train_rating, eval_rating, test_rating, adj_item, adj_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d2a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_kg(kg_np):\n",
    "    # 构造知识库字典\n",
    "    print('constructing knowledge graph ...')\n",
    "    kg = dict()\n",
    "    for triple in kg_np:\n",
    "        head = triple[0]\n",
    "        relation = triple[1]\n",
    "        tail = triple[2]\n",
    "        # treat the KG as an undirected graph\n",
    "        if head not in kg:\n",
    "            kg[head] = []\n",
    "        kg[head].append((tail, relation))\n",
    "        if tail not in kg:\n",
    "            kg[tail] = []\n",
    "        kg[tail].append((head, relation))\n",
    "    return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b317ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_adj(args, kg, entity_num):\n",
    "    print('constructing adjacency matrix ...')\n",
    "    maxIdx = max(list(kg.keys()))\n",
    "    # neighbor_sample_size -> 临近实体采样数 论文中为4\n",
    "    # each line of adj_entity stores the sampled neighbor entities for a given entity\n",
    "    adj_entity = np.zeros([maxIdx + 1, args.neighbor_sample_size], dtype=np.int64)\n",
    "    # each line of adj_relation stores the corresponding sampled neighbor relations\n",
    "    adj_relation = np.zeros([maxIdx + 1, args.neighbor_sample_size], dtype=np.int64)\n",
    "    for entity in range(maxIdx + 1):\n",
    "        if entity not in kg:\n",
    "            continue\n",
<<<<<<< HEAD
    "        neighbors = kg[entity]\n",
=======
    "        neighbors = kg[entity] \n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "        n_neighbors = len(neighbors)\n",
    "        if n_neighbors >= args.neighbor_sample_size:\n",
    "            sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.neighbor_sample_size, replace=False)\n",
    "        else:\n",
    "            sampled_indices = np.random.choice(list(range(n_neighbors)), size=args.neighbor_sample_size, replace=True)\n",
<<<<<<< HEAD
    "        adj_entity[entity] = np.array([neighbors[i][0] for i in sampled_indices])\n",
=======
    "        adj_entity[entity] = np.array([neighbors[i][0] for i in sampled_indices]) # 把采样到下标对一个的\n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "        adj_relation[entity] = np.array([neighbors[i][1] for i in sampled_indices])\n",
    "    return adj_entity, adj_relation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 7,
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
   "id": "689c8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kg(args):\n",
    "    kg_file = './data/' + args.dataset + '/kg_final'\n",
    "    if os.path.exists(kg_file + '.npy'):\n",
    "        kg_np = np.load(kg_file + '.npy')\n",
    "    else:\n",
    "        kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int64)\n",
    "        np.save(kg_file + '.npy', kg_np)\n",
    "        \n",
    "    n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "    n_relation = len(set(kg_np[:, 1]))\n",
    "    print(n_entity, n_relation)\n",
<<<<<<< HEAD
    "    kg = construct_kg(kg_np)\n",
=======
    "    kg = construct_kg(kg_np) # kg[entity] -> (relatedEntity, relationship)\n",
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
    "    adj_entity, adj_relation = construct_adj(args, kg, n_entity)\n",
    "    return n_entity, n_relation, adj_entity, adj_relation"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 名称      | 含义 |\n",
    "| ----------- | ----------- |\n",
    "| n_user      | 用户数       |\n",
    "| n_item   | 交互项目数        |\n",
    "| train_data   | 训练集        |\n",
    "| eval_data   | 验证集        |\n",
    "| test_data   | 测试集        |\n",
    "| adj_item   | 行:用户 列:用户产生的交互编号 每个用户采样20个交互        |\n",
    "| n_classes   | 待分类的结果数量 例如性别就是男、女2类        |\n",
    "| ratings_np   | 从ratings文件中读取的原始数据 numpy数组 每个用户一行 列分别是(用户id, 交互id, 性别)        |\n",
    "| adj_user   | adj_user[i][j]: 与用户i产生过的某个用户id (没有顺序, 是随机采样的) |\n",
    "| n_entity   | 知识库中实体的个数(头尾做union)        |\n",
    "| n_relation   | 知识库中关系的个数(头尾做union)        |\n",
    "| adj_entity   | adj_entity[i]: 与实体i有关系的实体集合(采样)        |\n",
    "| adj_relation   | adj_relation[i]: 与实体i有关系的关系(采样)        |"
   ]
  },
  {
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e29e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    n_user, n_item, train_data, eval_data, test_data, adj_item, n_classes, ratings_np, adj_user = load_rating(args)\n",
    "    n_entity, n_relation, adj_entity, adj_relation = load_kg(args)\n",
    "    print('data loaded.')\n",
    "\n",
    "    return n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, adj_entity, adj_relation, adj_item, n_classes, ratings_np, adj_user"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
   "id": "c4c0884b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "37473 24\n",
      "constructing knowledge graph ...\n",
      "constructing adjacency matrix ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "ret = load_data(args)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "d0190ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hkgcn",
   "language": "python",
   "name": "hkgcn"
=======
   "execution_count": 11,
   "id": "d0190ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, adj_entity, adj_relation, adj_item, n_classes, ratings_np, adj_user = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = np.array([n_user, n_item, n_entity, n_relation, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把处理好的数据存储到硬盘上给下个模块用\n",
    "\n",
    "def write_file(filename, obj):\n",
    "  with open('{}.npy'.format(filename), 'wb') as f:\n",
    "    np.save(f, obj)\n",
    "\n",
    "write_file('n_file', n_list)\n",
    "write_file('train_data', train_data)\n",
    "write_file('eval_data', eval_data)\n",
    "write_file('test_data', test_data)\n",
    "write_file('adj_entity', adj_entity)\n",
    "write_file('adj_relation', adj_relation)\n",
    "write_file('adj_item', adj_item)\n",
    "write_file('ratings_np', ratings_np)\n",
    "write_file('adj_user', adj_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0293982cc521ba79463c97bcbf6060ec5665b6d4557db7071378f3862020d1c"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('hkgcn': conda)",
   "name": "python3"
>>>>>>> 2d073fa (compelete collecting neighbor algorihtmn)
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
